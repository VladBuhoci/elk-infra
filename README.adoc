= ELK cluster for dev/learning
Quickly spin up an ELK cluster using Docker Compose.
:showtitle:
:toc: auto
:toclevels: 3

== What is it?

A set of configuration files which simplify the creation and setting up of a cluster with 3 Elasticsearch nodes, 1 Logstash instance and 1 Kibana instance respectively.

User authentication and TLS are both enabled by default, although this can be changed before starting the cluster.

== How to use it?

=== Prerequisites

You will need Docker and Compose to be able to spin up the ELK cluster.

=== Configure the cluster

Edit the files for your own needs.

[IMPORTANT]
====
Even though there are 2 user passwords in the .env file ("elastic" and "kibana_system"), only "elastic" is meant to be used by humans (to talk to Elasticsearch or to log into Kibana).
====

.In case you want one more ES node, adding it is fairly easy:
* add another service in the Compose file (copy & paste one of the existing ES config blocks);
* change the list of instances (config/certs/instances.yml in the Compose file) to also contain the new ES node;
* update the "environment" variables for the new node as well as the "cluster.initial_master_nodes" and "discovery.seed_hosts" env variables for every ES instance;
* add a new volume for the new ES node and use it in the "volumes" section of its config block;
* update Kibana's "ELASTICSEARCH_HOSTS" env variable by appending the new ES service and its unique(!) port (which should be added in the .env file).

=== Start the cluster

Whenever you use a new shell instance, source the setenv.sh script to export any dynamic env vars required by the Compose file.
[source,bash]
----
source setenv.sh
----

Now, simply run the following command to start the instances:
[source,bash]
----
docker compose up
----

Note that running it this way will attach your terminal to the processes and you will be able to view their logs in real time. Also, hitting Ctrl+C will terminate them.

To not have your terminal attached to the container processes, start them in detached mode:
[source,bash]
----
docker compose up -d
----

To check the status of your containers, execute this:
[source,bash]
----
docker compose ps
----

Logs can be viewed with the following command (add "-f" for real time logs and, optionally, specify one service, i.e., "kibana"):
[source,bash]
----
docker compose logs [-f] [name of service]
----

[NOTE]
====
You may encounter an error message in the Elasticsearch logs after starting the cluster, which will cause the processes to exit:

"max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]"

In this case, please run _init.sh_ from this project and try starting the cluster again:

[source,bash]
----
./init.sh
----

(thanks to https://github.com/justincormack/nsenter1 & https://stackoverflow.com/questions/51445846/elasticsearch-max-virtual-memory-areas-vm-max-map-count-65530-is-too-low-inc)
====

To stop the instances, run:
[source,bash]
----
docker compose down
----

[NOTE]
====
Volumes will not be automatically removed, in order to not lose data. You need to manally remove them if you no longer have any use for the stored data:

[source,bash]
----
docker compose down -v
----
====

=== Access Elasticsearch

TLS is enabled by default, so if it is left unchanged, you will require a CA certificate to talk to the ES nodes.

Get the cert using this command:
[source,bash]
----
docker compose cp elastic-01:/usr/share/elasticsearch/config/certs/ca/ca.crt ./elastic-ca.crt
----

Execute a test command now:
[source,bash]
----
# this needs to be run just once to get the "ELASTIC_PASSWORD" variable, unless the file gets updated later.
source .env

curl \
  --cacert ./elastic-ca.crt \
  -X GET \
  -s \
  -u "elastic:${ELASTIC_PASSWORD}" \
  -H "Content-Type: application/json" \
  https://localhost:9200/_cluster/health \
  | jq .
----

You should see something like this:
[source,json]
----
{
  "cluster_name": "docker-cluster",
  "status": "green",
  "timed_out": false,
  "number_of_nodes": 2,
  "number_of_data_nodes": 2,
  "active_primary_shards": 24,
  "active_shards": 48,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 0,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 100
}
----

=== Test Logstash

Note that this part depends entirely on the pipelines that are configured in the `./logstash/config/` and `./logstash/pipeline/` folders.

. Example #1
+
The _"http-json-only"_ pipeline only accepts JSON payloads as input (note that the _"message"_ key is mandatory):
+
[source,bash]
----
curl \
  -d '{"message": "{\"key1\": \"value1\", \"key2\": \"value2\"}"}' \
  http://localhost:1700/
----
+
The output can be found among Logstash's latest log lines:
+
[source,bash]
----
docker compose logs logstash | tail -n 6
----
+
You should get something like this:
+
[source]
----
logstash-1  | {
logstash-1  |     "newmsg" => {
logstash-1  |         "key2" => "value2",
logstash-1  |         "key1" => "value1"
logstash-1  |     }
logstash-1  | }
----

. Example #2
+
[IMPORTANT]
====
If this pipeline is tested for the first time, a custom user needs to be created because it is required by Logstash for storing data in an Elasticsearch index.

A script was made to simplify this step and to serve as a starting point for managing other user accounts:

[source,bash]
----
./init_logstash_creds.sh
----
====
+
The _"http-apache"_ pipeline was designed for log lines generated by an Apache server (sample data can be found in the `./test-data/` folder):
+
[source,bash]
----
curl \
  -d '184.252.108.229 - - [20/Sep/2017:13:22:22 +0200] "GET /products/view/123 HTTP/1.1" 200 12798 "https://testhost.com/products" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36"' \
  http://localhost:1701/
----
+
The output can be found among Logstash's latest log lines (note that this pipeline first outputs the filtered data to Elasticsearch and only then to stdout, if everything went okay) using the following command:
+
[source,bash]
----
docker compose logs logstash | tail -n 57
----
+
You should get something like this:
+
[source]
----
logstash-1  | {
logstash-1  |     "@timestamp" => 2017-09-20T11:22:22.000Z,
logstash-1  |           "http" => {
logstash-1  |         "response" => {
logstash-1  |                    "body" => {
logstash-1  |                 "bytes" => 12798
logstash-1  |             },
logstash-1  |             "status_code" => 200
logstash-1  |         },
logstash-1  |          "request" => {
logstash-1  |               "method" => "GET",
logstash-1  |             "referrer" => "https://testhost.com/products"
logstash-1  |         },
logstash-1  |          "version" => "1.1"
logstash-1  |     },
logstash-1  |       "@version" => "1",
logstash-1  |         "source" => {
logstash-1  |         "address" => "184.252.108.229"
logstash-1  |     },
logstash-1  |            "url" => {
logstash-1  |         "original" => "/products/view/123"
logstash-1  |     },
logstash-1  |     "client_geo" => {
logstash-1  |         "mmdb" => {
logstash-1  |             "dma_code" => 819
logstash-1  |         },
logstash-1  |          "geo" => {
logstash-1  |                 "country_name" => "United States",
logstash-1  |              "region_iso_code" => "US-WA",
logstash-1  |                     "location" => {
logstash-1  |                 "lat" => 47.2565,
logstash-1  |                 "lon" => -122.4421
logstash-1  |             },
logstash-1  |                  "region_name" => "Washington",
logstash-1  |                  "postal_code" => "98493",
logstash-1  |               "continent_code" => "NA",
logstash-1  |             "country_iso_code" => "US",
logstash-1  |                     "timezone" => "America/Los_Angeles",
logstash-1  |                    "city_name" => "Tacoma"
logstash-1  |         },
logstash-1  |           "ip" => "184.252.108.229"
logstash-1  |     },
logstash-1  |     "user_agent" => {
logstash-1  |               "os" => {
logstash-1  |                "full" => "Windows 10",
logstash-1  |                "name" => "Windows",
logstash-1  |             "version" => "10"
logstash-1  |         },
logstash-1  |         "original" => "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36",
logstash-1  |           "device" => {
logstash-1  |             "name" => "Other"
logstash-1  |         },
logstash-1  |             "name" => "Chrome",
logstash-1  |          "version" => "60.0.3112.90"
logstash-1  |     },
logstash-1  |           "type" => "access"
logstash-1  | }
----
+
[NOTE]
====
If you want to visualize the data in Kibana, you can do so by going to the _Analytics / Discover_ page and selecting the appropriate data view.

* you might have to create one if it is the first time this step is performed. 

Then adjust the date/time range accordingly (pay attention to the Logstash events' timestamps!).
====

=== Access Kibana

After every container process had been stabilised, Kibana becomes available at http://localhost:5601 by default (the port can be changed in the .env file).

Log in with the "elastic" user account and your chosen password (also specified in the .env file).

== What now?

If everything went well up to this point, then congrats! Feel free to change the cluster to suit your needs.

*_Happy learning!_*

